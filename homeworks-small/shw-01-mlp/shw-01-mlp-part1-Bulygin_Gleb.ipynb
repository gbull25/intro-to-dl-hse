{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lonely-delta",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Глубинное обучение 1 / Введение в глубинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1. Часть 1: автоматическое дифференцирование.\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "Оценка после штрафа после мягкого дедлайна вычисляется по формуле $M_{\\text{penalty}} = M_{\\text{full}} \\cdot 0.85^{t/1440}$, где $M_{\\text{full}}$ — полная оценка за работу без учета штрафа, а $t$ — время в минутах, прошедшее после мягкого дедлайна (округление до двух цифр после запятой). Таким образом, спустя первые сутки после мягкого дедлайна вы не можете получить оценку выше 8.5, а если сдать через четыре дня после мягкого дедлайна, то ваш максимум — 5.22 балла.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать свой фреймворк для обучения нейронных сетей на основе `numpy`. Интерфейс фреймворка будет максимально копировать PyTorch, так что вы немного познакомитесь с тем, как все устроено изнутри. Директория `modules` содержит файлы с шаблонами фреймровка, а `tests` &mdash; тесты для проверки корректности ваших реализаций.\n",
    "\n",
    "Ячейка ниже повзоляет переподгружать питоновские модули, которые вы изменили после импорта, без необходимости перезапускать ноубук."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suspended-death",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lonely-component",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-mechanics",
   "metadata": {},
   "source": [
    "## 0. Автоматическое дифференцирование\n",
    "\n",
    "Самый главным объектом в нашем фреймворке будет абстракция слоя (класс `Module`), которая реализована в файле `modules/base.py`. Перед тем как писать свой код, ознакомьтесь с реализацией класса `Module`. Каждый слой должен поддерживать две операции: проход вперед, который принимает выход предыдущего слоя и вычисляет функцию слоя, и проход назад, который принимает выход предыдущего слоя и производную по своему выходу, а возвращает производную по входу, попутно обновляя градиент по своим параметрам. Вспомним общую схему еще раз. Пусть $f(x, w)$ &mdash; это наша функция слоя, которая зависит от входа $x$ и параметров в $w$, $\\ell$ - функция потерь, градиент по параметрам которой нас интересует. Тогда:\n",
    "\n",
    "- Проход вперед:\n",
    "\n",
    "$$y = f(x, w)$$\n",
    "\n",
    "- Проход назад:\n",
    "\n",
    "$$\\frac{d\\ell}{dx} = \\frac{d\\ell}{dy} \\cdot \\frac{df(x, w)}{dx}$$\n",
    "\n",
    "$$\\frac{d\\ell}{dw} = \\frac{d\\ell}{dy} \\cdot \\frac{df(x, w)}{dw}$$\n",
    "\n",
    "Таким образом, при проходе вперед в слой передается $x$, при проходе назад $x$ и $\\frac{d\\ell}{dy}$. Кроме того, каждый слой сохраняет свой выход при проходе вперед, чтобы затем передать его в следующий слой при проходе назад. Сответственно, базовый класс `Module` реализует функции `forward` (или его alias, метод `__call__`, аналогично тому, как это сделано в PyTorch) и `backward`. Кроме того, шаблоны содержат некоторые служебные функции, в том числе `train` и `eval`, которые меняют режим слоя. Все слои, которые вам нужно реализовать, будут наследоваться от класса `Module`. В них потребуется реализовать методы `compute_output`, `compute_grad_input` и `update_grad_parameters` (если у слоя есть обучаемые параметры). За подробностями обращайтесь к док-строкам в шаблонах. Мы будем реализовывать слои аналогично соответствующим слоям из PyTorch, поэтому можете сверяться с документацией для уточнения значений параметров слоев. Для вашего удобства мы приводим тесты для отладки реализаций, ваше решение должно их проходить (баллы начисляются за пройденные тесты). В случае возникновения затруднений советуем дебажить код, сравнивая вашу реализацию с модулями из PyTorch.\n",
    "\n",
    "**Важно:** мы хотим получить такое же поведение, как у PyTorch, поэтому если сделать несколько проходов назад без вызова функции `zero_grad`, то градиенты со всех проходов назад должны суммироваться. Это значит, что в функции `update_grad_parameters` нужно прибавить новый градиент к уже имеющемуся, а не перезаписать его.\n",
    "\n",
    "Обратите внимание, что все ваши подсчеты функций и градиентов должны быть **векторизованными**, то есть включать только операции на `numpy`/`scipy` и **никаких питоновских циклов** (или оберток над ними из указанных библиотек). Специальные места, в которых циклы разрешены, будут указаны отдельно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-vietnam",
   "metadata": {},
   "source": [
    "## 1. Линейный слой (1 балл)\n",
    "\n",
    "- Прототип: [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- Расположение: `modules.layers.Linear`\n",
    "\n",
    "Отныне мы будем иметь в виду, что вход нейросети $x$ имеет размер $B \\times N$, где $B$ &mdash; размер мини-батча, а $N$ &mdash; размерность. Функция слоя выглядит как:\n",
    "\n",
    "$$\n",
    "y = x \\, W^T + b,\n",
    "$$\n",
    "\n",
    "где $W \\in \\mathbb{R}^{M \\times N}, b \\in \\mathbb{R}^M$. Таким образом, выход слоя имеет размер $B \\times M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hired-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_linear ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-satellite",
   "metadata": {},
   "source": [
    "## 2. Batch-нормализация (2.5 балла)\n",
    "\n",
    "- Прототип: [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)\n",
    "- Расположение: `modules.layers.BatchNormalization`\n",
    "\n",
    "Batch-нормализация &mdash; первый слой, который работает по-разному в train и eval режимах.\n",
    "\n",
    "**Режим train:**\n",
    "\n",
    "1. Для каждой координаты входа считаем статистики по мини-батчу ($x_i \\in \\mathbb{R}^N$ &mdash; один объект в мини-батче):\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{B} \\sum_{i=1}^B x_i, \\quad\\quad \\mu \\in \\mathbb{R}^N \\\\\n",
    "\\sigma^2 = \\frac{1}{B} \\sum_{i=1}^B (x_i - \\mu)^2, \\quad\\quad \\sigma^2 \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "Обратите внимание, что здесь используется **смещенная** оценка дисперсии (то есть мы делим сумму квадратов отклонений на $B$, а не на $B-1$).\n",
    "\n",
    "2. Нормируем вход с учетом статистик:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "3. Применяем афинное преобразование к нормированному входу (если `affine = True`), умножение поэлементное. Это и будет выход слоя.\n",
    "\n",
    "$$\n",
    "y_i = \\hat{x}_i * w + b, \\quad\\quad w, b \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "4. Обновляем бегущие статистики слоя:\n",
    "\n",
    "$$\n",
    "\\text{running_mean} = (1 - \\text{momentum}) \\cdot \\text{running_mean} + \\text{momentum} \\cdot \\mu \\\\\n",
    "\\text{running_var} = (1 - \\text{momentum}) \\cdot \\text{running_var} + \\text{momentum} \\cdot \\frac{B}{B - 1}\n",
    "\\cdot \\sigma^2\n",
    "$$\n",
    "\n",
    "Здесь перенормировка $\\sigma^2$ необходима, чтобы обновлять бегущую дисперсию **несмещенной** оценкой (точно так же это реализовано в PyTorch).\n",
    "\n",
    "К параметрам слоя, которые обновляются градиентом, относятся только $w$ (`weight`) и $b$ (`bias`), но не `running_mean` и `running_var`.\n",
    "\n",
    "**Режим eval:**\n",
    "\n",
    "1. Нормируем вход, используя бегущие статистики:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\text{running_mean}}{\\sqrt{\\text{running_var} + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "2. Применяем афинное преобразование к нормированному входу:\n",
    "\n",
    "$$\n",
    "y_i = \\hat{x}_i * w + b\n",
    "$$\n",
    "\n",
    "**Хозяйке на заметку**\n",
    "\n",
    "- Убедитесь, что проход назад корректно работает и для train, и для eval режимов\n",
    "- Сохраняйте промежуточные вычисления при проходе вперед, чтобы переиспользовать их при проходе назад\n",
    "- Весьма вероятно, что у вас не получится правильная реализация с первого раза. Не отчаивайтесь, автор задания тоже потратил не один час, пока этот модуль не заработал. Если чувствуете, что зашли в тупик, то пользоваться гуглом никто не запрещал, но не забудьте указать источники, которыми пользуетесь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "every-prison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bn ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_bn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-dover",
   "metadata": {},
   "source": [
    "## 3. Dropout (1 балл)\n",
    "\n",
    "- Прототип: [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
    "- Расположение: `modules.layers.Dropout`\n",
    "\n",
    "Dropout &mdash; еще один слой, чье поведение различно в train и eval режимах. Поведения слоя регулируется параметром $p$ &mdash; вероятностью занулить координату входа.\n",
    "\n",
    "**Режим train:**\n",
    "\n",
    "Обозначим за $m$ бинарную маску, имеющую такой же размер, как вход $x$. Маска генерируется по правилу $m_{ij} \\sim \\text{Bernoulli}(1-p)$. При этом при каждом новом проходе вперед генерируется новая маска (то есть она не фиксирована). Функция слоя (умножение поэлементное):\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1-p} m * x\n",
    "$$\n",
    "\n",
    "Нормализация на $1-p$ необходима, чтобы среднее значение нейронов входа не изменилось.\n",
    "\n",
    "**Режим eval:**\n",
    "\n",
    "Здесь все предельно просто: вход слоя никаких не изменяется $y=x$.\n",
    "\n",
    "**Хозяйке на заметку**\n",
    "\n",
    "- Убедитесь, что проход назад корректно работает и для train, и для eval режимов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collective-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dropout ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-metropolitan",
   "metadata": {},
   "source": [
    "## 4. Функции активации (1.5 балла)\n",
    "\n",
    "### ReLU\n",
    "\n",
    "- Прототип: [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Расположение: `modules.activations.ReLU`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y = \\max(x, 0)\n",
    "$$\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "- Прототип: [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html?highlight=nn%20sigmoid#torch.nn.Sigmoid)\n",
    "- Расположение: `modules.activations.Sigmoid`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### Softmax\n",
    "\n",
    "- Прототип: [nn.Softmax](http://bit.ly/get3a)\n",
    "- Расположение: `modules.activations.Softmax`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y_{ij} = \\frac{\\exp(x_{ij})}{\\sum_{k=1}^{N} \\exp(x_{ik})}\n",
    "$$\n",
    "\n",
    "### LogSoftmax\n",
    "\n",
    "- Прототип: [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html?highlight=log%20softmax#torch.nn.LogSoftmax)\n",
    "- Расположение: `modules.activations.LogSoftmax`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y_{ij} = \\log \\left(\\frac{\\exp(x_{ij})}{\\sum_{k=1}^{N} \\exp(x_{ik})}\\right)\n",
    "$$\n",
    "\n",
    "**Хозяйке на заметку**\n",
    "\n",
    "- Пользуйтесь функциями из `scipy.special`\n",
    "- Реализовывать `LogSoftmax` как логарифм от модуля `Softmax` &mdash; плохая идея"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "great-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_activations ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-proof",
   "metadata": {},
   "source": [
    "## 5. Контейнер Sequential (1 балл)\n",
    "\n",
    "- Прототип: [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "- Расположение: `modules.layers.Sequential`\n",
    "\n",
    "Контейнер-обертка, который применяет слои последовательно.\n",
    "\n",
    "**Важно:** здесь разрешен цикл по модулям при проходах вперед и назад."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "standard-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sequential ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-mirror",
   "metadata": {},
   "source": [
    "## 6. Функции потерь (1 балл)\n",
    "\n",
    "Функции потерь отличаются от всех остальных модулей тем, что являются стоком вычислительного графа (то есть из них нет исходящих операций). Это означает, что с них начинается проход назад, поэтому интерфейс функции `compute_grad_input` выглядит иначе: вместо входа модуля и производной по выходу в функцию приходит предсказание нейронной сети (производная по которому нас и интересует, чтобы запустить проход назад) и целевая переменная. Базовый класс для всех функций потерь &mdash; `modules.base.Criterion`.\n",
    "\n",
    "### MSE\n",
    "\n",
    "- Прототип: [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "- Расположение: `modules.criterions.MSELoss`\n",
    "\n",
    "Пусть $f \\in \\mathbb{R}^{B\\times N}$ &mdash; предсказание нейронной сети, а $y \\in \\mathbb{R}^{B\\times N}$ &mdash; целевая переменная. Функция потерь выглядит как:\n",
    "\n",
    "$$\n",
    "\\ell(f, y) = \\frac{1}{BN} \\sum_{i=1}^B \\sum_{j=1}^N (f_{ij} - y_{ij})^2\n",
    "$$\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "- Прототип: [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- Расположение: `modules.criterions.CrossEntropyLoss`\n",
    "\n",
    "Кросс-энтропия &mdash; это функция потерь для обучения классификаторов. Пусть $f \\in \\mathbb{R}^{B\\times C}$ (где $C$ &mdash; число классов) &mdash; предсказание нейронной сети (это так называемые *логиты*, обычно выходы линейного слоя без активации, потому могут быть любыми вещественными числами), а $y \\in \\{1, \\dots, C\\}^B$ &mdash; целевая переменная (номер класса соответствующего объекта). Функция потерь вычисляется как:\n",
    "\n",
    "$$\n",
    "p_{ic} = \\frac{\\exp(f_{ic})}{\\sum_{k=1}^C \\exp(f_{ik})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ell(f, y) = -\\frac{1}{B} \\sum_{i=1}^B \\sum_{c=1}^C [c = y_i] \\log p_{ic}\n",
    "$$\n",
    "\n",
    "При этом, $p_{ic}$ &mdash; это вероятность класса $с$ для объекта $i$, которую предсказывает нейронная сеть.\n",
    "\n",
    "**Важно:** вычисление Softmax, а затем логарифма от него &mdash; численно нестабильная операция. Воспользуйтесь слоем LogSoftmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "early-hampshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_criterions ... "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tests\u001b[38;5;241m.\u001b[39mtest_criterions()\n",
      "File \u001b[1;32mc:\\Users\\Midnight Commander\\Documents\\Study\\DL\\intro-to-dl-hse-2022-2023\\homeworks-small\\shw-01-mlp\\tests\\test_criterions.py:51\u001b[0m, in \u001b[0;36mtest_criterions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_shape \u001b[38;5;129;01min\u001b[39;00m input_shapes:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mse \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 51\u001b[0m         test_criterion(\n\u001b[0;32m     52\u001b[0m             input_shape, mse\u001b[38;5;241m=\u001b[39mmse, outer_iters\u001b[38;5;241m=\u001b[39mnum_tests,\n\u001b[0;32m     53\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m random_seed\n\u001b[0;32m     54\u001b[0m         )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Midnight Commander\\Documents\\Study\\DL\\intro-to-dl-hse-2022-2023\\homeworks-small\\shw-01-mlp\\tests\\test_criterions.py:39\u001b[0m, in \u001b[0;36mtest_criterion\u001b[1;34m(input_shape, mse, outer_iters, random_seed)\u001b[0m\n\u001b[0;32m     36\u001b[0m x2\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     38\u001b[0m l1 \u001b[38;5;241m=\u001b[39m module1(x1, y1)\n\u001b[1;32m---> 39\u001b[0m l2 \u001b[38;5;241m=\u001b[39m module2(x2, y2)\n\u001b[0;32m     40\u001b[0m assert_almost_equal(l1, l2\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), debug_msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward pass: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m l2\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1186\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1187\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "tests.test_criterions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-edition",
   "metadata": {},
   "source": [
    "## 7. Оптимизаторы (1.5 балла)\n",
    "\n",
    "Оптимизатор &mdash; вспомогательный класс, который обновляет веса нейронной сети при градиентном спуске, используя сохраненные градиенты параметров. Базовый класс &mdash; `modules.base.Optimizer`. В документации PyTorch приведен псевдокод с описанием алгоритмов, советуем обратиться туда.\n",
    "\n",
    "**Важно:** здесь разрешен цикл по параметрам и градиентам (см. шаблоны)\n",
    "\n",
    "### SGD\n",
    "\n",
    "- Прототип: [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "- Расположение: `modules.optimizers.SGD`\n",
    "\n",
    "### Adam\n",
    "\n",
    "- Прототип: [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "- Расположение: `modules.criterions.CrossEntropyLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "selected-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_optimizers ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-therapist",
   "metadata": {},
   "source": [
    "## 8. DataLoader (0.5 балла)\n",
    "\n",
    "- Прототип: [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- Расположение: `modules.dataloader.DataLoader`\n",
    "\n",
    "И последнее, что нам осталось реализовать &mdash; это DataLoader, который перемешивает данные раз в эпоху (если это необходимо) и формирует из них мини-батчи. Технически, это будет питоновский итератор. Вот краткое [руководство](https://stackoverflow.com/questions/19151/how-to-build-a-basic-iterator), как написать итератор.\n",
    "\n",
    "Обратите внимание, что ваша реализация должна уметь работать как с одномерным массивом целевой переменной (с формой `(num_samples, )` &mdash; так будет удобнее учить нейронную сеть на кросс-энтропию), так и с двумерной версией (с формой`(num_samples, 1)` &mdash; сответственно, на MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ready-trance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataloader ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-algorithm",
   "metadata": {},
   "source": [
    "## Собираем все вместе\n",
    "\n",
    "Если вы все сделали правильно, то следующий кусок кода с обучением нейронной сети должен заработать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "innovative-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modules as mm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "super-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train = np.random.randn(2048, 8)\n",
    "X_test = np.random.randn(512, 8)\n",
    "y_train = np.sin(X_train).sum(axis=1, keepdims=True)\n",
    "y_test = np.sin(X_test).sum(axis=1, keepdims=True)\n",
    "\n",
    "train_loader = mm.DataLoader(X_train, y_train, batch_size=64, shuffle=True)\n",
    "test_loader = mm.DataLoader(X_test, y_test, batch_size=64, shuffle=False)\n",
    "\n",
    "model = mm.Sequential(\n",
    "    mm.Linear(8, 32),\n",
    "    mm.BatchNormalization(32),\n",
    "    mm.ReLU(),\n",
    "    mm.Linear(32, 64),\n",
    "    mm.Dropout(0.25),\n",
    "    mm.Sigmoid(),\n",
    "    mm.Linear(64, 1)\n",
    ")\n",
    "optimizer = mm.Adam(model, lr=1e-2)\n",
    "criterion = mm.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "gothic-latin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a142f0be1e5f4eabad131444c9e3765e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "pbar = tqdm(range(1, num_epochs + 1))\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        model.backward(X_batch, criterion.backward(predictions, y_batch))\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    model.eval()\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        test_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    train_loss /= train_loader.num_samples()\n",
    "    test_loss /= test_loader.num_samples()\n",
    "    pbar.set_postfix({'train loss': train_loss, 'test loss': test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-district",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
